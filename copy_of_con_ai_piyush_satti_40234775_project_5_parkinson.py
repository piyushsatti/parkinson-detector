# -*- coding: utf-8 -*-
"""Copy of Con_AI_Piyush_Satti_40234775_Project_5_Parkinson.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q78GjK-4HQn9FiAwVGEONqHe9ojIc06V

# Detecting Parkinson’s disease from short speech recordings
## Project 5

The goal of this project is to develop a neural network that can detect the presence of Parkinson's disease as accurately as possible starting from a short speech recording.

![Screenshot 2025-04-19 at 3.28.00 AM.png](data:image/png;base64,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)

The plan is to implement this system using SpeechBrain. You will start from basic models available off-the-shelf (e.g. Xvectors and ECAPA-TDNN) and you will progressively make the model more complex and, hopefully, better performing. We will use this dataset.
Students are encouraged to follow these steps:

1. The first step is to do an extensive literature review on machine learning methods used to detect Parkinson's disease. This can give us a better idea on the state-of-the-art technology, adopted techniques, datasets, etc. It is important to extend the literature review to other diseases related to brain injuries such as Alzheimer’s and concussions.
2. After downloading the dataset, you can start exploring proper machine learning models for them. We can first explore models like Xvectors and ECAPA-TDNN (there are some similar recipes already available in SpeechBrain, for instance https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonLanguage).
3. Then you should consider fine-tuning self-supervised models such as wav2vec2, Hubert, and WavLM. Also in this case, similar recipes are available in SpeechBrain (https://github.com/speechbrain/speechbrain/tree/develop/recipes/IEMOCAP/emotion_recognition).
4. Finally, you should consider other strategies to improve the performance, such as better models, transfer learning strategies, data augmentation, hyperparameter tuning.

## Persistent Storage (Optional)
We will connect to drive to save results between run sessions. This is an optional step.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Env Vars"""

import torch, warnings

DATASET_PATH = "/content/dataset"
PROJECT_PATH = "/content/drive/MyDrive/Colab Notebooks/Con. AI COMP 691/Con. AI Project"
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

warnings.filterwarnings("ignore", category=UserWarning, module="torchaudio")

"""# Exploratory Data Analysis
The goal of this section is to explore how the dataset looks like. In terms of both qualitative and quantitative measures.
"""

# Fetch the dataset
!wget "https://huggingface.co/datasets/birgermoell/Italian_Parkinsons_Voice_and_Speech/resolve/main/italian_parkinson/Italian%20Parkinson's%20Voice%20and%20speech.zip?download=true" -O dataset.zip

# Extract the dataset
!unzip dataset.zip -d "{DATASET_PATH}"

"""## Qualitative
In this section we determine how the data looks like qualitatively.

### Q1. What is the folder structure?

### Q2. What are the labels available to us?

### Q3. What does the audio look like?

#### A1. Folder Structure
The dataset has 3 folders:

```
dataset/
└── 15 Young Healthy Control/
    ├── Alberto R/
    │   ├── B1LBULCAAS94M1001.wav
    │   ├── B2LBULCAAS94M1001.wav
    │   ├── PR1LBULCAAS94M1001.wav
    │   └── PR1LBULCAAS94M1002.wav
    ├── Alessandro M/
    ├── Alessandro P/
    └── Arianna P/
└── 22 Elderly Healthy Control/
    ├── ...
└── 28 People with Parkinson's Disease/
    ├── ...
```
"""

import os

audio_files = []

for root, dirs, files in os.walk(DATASET_PATH):
    for file in files:
        if file.lower().endswith('.wav'):
            audio_files.append(os.path.join(root, file))

print("Top 5 files:")
[print(x) for x in audio_files[:5]]

print("\nMiddle 5 files:")
[print(x) for x in audio_files[50:55]]

print("\nBottom 5 files:")
[print(x) for x in audio_files[-5:]]

print() # To clear the print buffer

"""#### A2. Available Labels
From the above folder structure it is obvious taht the labels we can extract are as following:

1. Unique `identifier` for each file given by the name of the .wav file.
2. `Young Healthy`, `Elderly healthy`, and `Parkinson's Disease` given by the name of the parent folder.
3. The name of the Individual given by the folder containing the .wav files.

#### A3. Analysis of the Audio File
The audio files seem to be prununciation of some italian words. They vary in length but all have the same sampling rate.

Detailed Audio Exploration Summary

- **File**: `path_to_audio_file.wav`
- **Channels**: Mono (`1`)
- **Sample Rate**: `16000 Hz`
- **Duration**: `59.68 seconds`

Observations:
- **Waveform** shows typical amplitude range and clear audio patterns.
- **Spectrogram** illustrates clear frequency components.
- **Mel Spectrogram** provides distinct spectral energy distribution, beneficial for ML feature extraction.
- **MFCC** captures timbral characteristics clearly, making it highly suitable for classification tasks such as Parkinson's disease detection.
"""

import torchaudio

audio_path = audio_files[9]  # Your audio file path

# Load audio
waveform, sample_rate = torchaudio.load(audio_path)

# Print detailed information
print(f"Audio File: {audio_path}")
print(f"Waveform Shape: {waveform.shape}")
print(f"Channels: {waveform.shape[0]}")
print(f"Sample Rate: {sample_rate}")
print(f"Duration: {waveform.shape[1] / sample_rate:.2f} seconds")

import IPython.display as ipd

# Audio Playback
ipd.Audio(waveform.numpy(), rate=sample_rate)

import matplotlib.pyplot as plt

# Plot waveform clearly
plt.figure(figsize=(12, 4))
plt.plot(waveform.t().numpy(), color='blue')
plt.title('Audio Waveform')
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.grid(True)
plt.show()

# Generate and plot spectrogram clearly
spectrogram = torchaudio.transforms.Spectrogram()(waveform)

plt.figure(figsize=(12, 4))
plt.imshow(spectrogram.log2()[0,:,:].numpy(), cmap='viridis', aspect='auto', origin='lower')
plt.title('Spectrogram (Log-scaled)')
plt.ylabel('Frequency bins')
plt.xlabel('Time frames')
plt.colorbar(label='Log magnitude')
plt.show()

# Mel Spectrogram clearly visualized
mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(waveform)

plt.figure(figsize=(12, 4))
plt.imshow(mel_spectrogram.log2()[0,:,:].numpy(), cmap='inferno', aspect='auto', origin='lower')
plt.title('Mel Spectrogram (Log-scaled)')
plt.ylabel('Mel frequency bins')
plt.xlabel('Time frames')
plt.colorbar(label='Log magnitude')
plt.show()

# MFCC clearly visualized (very common ML feature)
mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate)(waveform)

plt.figure(figsize=(12, 4))
plt.imshow(mfcc[0].detach().numpy(), cmap='coolwarm', aspect='auto', origin='lower')
plt.title('MFCC')
plt.ylabel('Coefficients')
plt.xlabel('Time frames')
plt.colorbar(label='Coefficient magnitude')
plt.show()

"""## Quantitative

### How many data points are available to us?

### Dataset Structure

- **Total Audio Files**: 831
- **Number of Participants**: 61
- **Audio Format**: `.wav`
"""

import os

audio_files = []

for root, dirs, files in os.walk(DATASET_PATH):
    for file in files:
        if file.lower().endswith('.wav'):
            audio_files.append(os.path.join(root, file))

# Display total number of audio files
print(f"Total .wav files found: {len(audio_files)}")

from collections import defaultdict

participant_files_count = defaultdict(int)

for file_path in audio_files:
    participant_name = os.path.basename(os.path.dirname(file_path))
    participant_files_count[participant_name] += 1

print("\nTotal no. of participants: " + str(len(participant_files_count)))

"""### Aggreagate Data Analytics

#### Aggregate Waveform Analysis Summary

| Feature             | Healthy (mean ± std) | Parkinson (mean ± std) |
|---------------------|----------------------|------------------------|
| Duration (seconds)  | XX ± YY              | AA ± BB                |
| Mean Amplitude      | XX ± YY              | AA ± BB                |
| RMSE                | XX ± YY              | AA ± BB                |

### Observations:
- Healthy participants exhibit [clearly describe trends].
- Parkinson participants show [describe differences clearly].
- Spectral analysis reveals [summarize findings clearly].

These aggregate findings help in clearly distinguishing characteristics for the model to effectively classify Parkinson's versus healthy controls.

"""

import os
import pandas as pd

audio_data = []

for root, dirs, files in os.walk(DATASET_PATH):
    for file in files:
        if file.lower().endswith('.wav'):
            label = 'Healthy' if 'healthy' in root.lower() else 'Parkinson'
            audio_data.append({
                'filepath': os.path.join(root, file),
                'label': label
            })

audio_df = pd.DataFrame(audio_data)
print(audio_df.head())

import torchaudio
import numpy as np

def compute_audio_stats(filepath):
    waveform, sr = torchaudio.load(filepath)
    duration = waveform.shape[1] / sr
    amplitude_mean = waveform.abs().mean().item()
    rmse = torch.sqrt(torch.mean(waveform**2)).item()
    return duration, amplitude_mean, rmse

# Add stats to dataframe
durations, amplitudes, rmses = [], [], []

for path in audio_df['filepath']:
    duration, amplitude, rmse = compute_audio_stats(path)
    durations.append(duration)
    amplitudes.append(amplitude)
    rmses.append(rmse)

audio_df['duration_sec'] = durations
audio_df['amplitude_mean'] = amplitudes
audio_df['rmse'] = rmses

print(audio_df.head())

aggregate_stats = audio_df.groupby('label').agg({
    'duration_sec': ['mean', 'std'],
    'amplitude_mean': ['mean', 'std'],
    'rmse': ['mean', 'std']
})

print(aggregate_stats)

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

# Boxplot for durations
plt.figure(figsize=(8,5))
sns.boxplot(data=audio_df, x='label', y='duration_sec')
plt.title('Distribution of Audio Durations by Label')
plt.xlabel('Label')
plt.ylabel('Duration (sec)')
plt.show()

# Boxplot for amplitude
plt.figure(figsize=(8,5))
sns.boxplot(data=audio_df, x='label', y='amplitude_mean')
plt.title('Distribution of Mean Amplitude by Label')
plt.xlabel('Label')
plt.ylabel('Mean Amplitude')
plt.show()

# Boxplot for RMSE
plt.figure(figsize=(8,5))
sns.boxplot(data=audio_df, x='label', y='rmse')
plt.title('Distribution of RMSE by Label')
plt.xlabel('Label')
plt.ylabel('RMSE')
plt.show()

"""# Training Models

## Requirements
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install speechbrain

"""## Loading Dataset"""

import os
import torchaudio

def parse_wav_folder(root_dir: str) -> dict:

  assert isinstance(root_dir, str), f"Expected str, got {type(root_dir)}"

  parsed_data = {}

  for dirpath, _, filenames in os.walk(root_dir):

    for file in filenames:

      if not file.endswith(".wav"):
        continue

      path = os.path.join(dirpath, file)
      key = file.replace(".wav", "")

      if "28 People with Parkinson's disease" in path:
        label = "parkinson"
      elif any(x in path for x in ["15 Young Healthy Control", "22 Elderly Healthy Control"]):
        label = "not_parkinson"
      else:
        raise ValueError(f"Unknown path pattern: {path}")

      info = torchaudio.info(path)
      sample_rate = info.sample_rate

      parsed_data[key] = {
        "wav": path,
        "length": info.num_frames / sample_rate,
        "label": label
      }

  return parsed_data

ds = parse_wav_folder(os.path.join(PROJECT_PATH, "Italian_Parkinson_Voice_and_speech"))

"""### Train, Test, Valid split"""

from sklearn.model_selection import train_test_split

def split_dataset_dict(data: dict, val_ratio=0.2, test_ratio=0.2, seed=42):

  assert isinstance(data, dict), f"Expected dict, got {type(data)}"

  keys = list(data.keys())
  labels = [data[k]["label"] for k in keys]

  train_keys, test_keys = train_test_split(
    keys, test_size=test_ratio, stratify=labels, random_state=seed
  )

  train_labels = [data[k]["label"] for k in train_keys]

  train_keys, val_keys = train_test_split(
    train_keys, test_size=val_ratio / (1 - test_ratio), stratify=train_labels, random_state=seed
  )

  def build_subset(keys_subset):
    return {k: data[k] for k in keys_subset}

  return {
    "train": build_subset(train_keys),
    "valid": build_subset(val_keys),
    "test": build_subset(test_keys)
  }

ds = split_dataset_dict(ds)

"""### Creating train, test, validate json


"""

import json

def save_splits_to_json(splits: dict, output_dir: str):

  assert isinstance(splits, dict), f"Expected dict, got {type(splits)}"

  os.makedirs(output_dir, exist_ok=True)

  for split_name, data in splits.items():

    out_path = os.path.join(output_dir, f"{split_name}.json")

    with open(out_path, "w") as f:
      json.dump(data, f, indent=2)

save_splits_to_json(ds, PROJECT_PATH)

"""# Training Neural Networks

## Challenges Overcome

### Problem 1: Ran out of GPU Memory

### Problem 2: Not enough GPU time

## Speechbrain Code Files

### Xvector
"""

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file xvector_hparams.yaml
# 
# seed: 1986
# __set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]
# 
# project_folder: !PLACEHOLDER
# output_folder: !ref <project_folder>/results/xvector/<seed>
# save_folder: !ref <output_folder>/save
# train_log: !ref <output_folder>/train_log.txt
# 
# train_annotation: !ref <project_folder>/train.json
# valid_annotation: !ref <project_folder>/valid.json
# test_annotation:  !ref <project_folder>/test.json
# 
# chunk_duration: 20.0
# orig_sample_rate: 16000
# sample_rate: 8000
# number_of_epochs: 30
# batch_size: 16
# lr_start: 0.001
# lr_final: 0.00001
# n_classes: 2
# emb_dim: 256
# n_fea: 80
# 
# train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
#   save_file: !ref <train_log>
# 
# error_stats: !name:speechbrain.utils.metric_stats.MetricStats
#   metric: !name:speechbrain.nnet.losses.classification_error
#     reduction: batch
# 
# dataloader_options:
#   batch_size: !ref <batch_size>
#   shuffle:    True
# 
# noise_transform: !new:speechbrain.augment.time_domain.AddNoise
#   snr_low:  10
#   snr_high: 20
# 
# speed_transform: !new:speechbrain.augment.time_domain.SpeedPerturb
#   orig_freq: !ref <sample_rate>
#   speeds:    [0.9, 1.0, 1.1]
# 
# feature_extractor: !new:speechbrain.lobes.features.Fbank
#   n_mels: !ref <n_fea>
# 
# xvector: !new:speechbrain.lobes.models.Xvector.Xvector
#   activation: !name:torch.nn.LeakyReLU
#   in_channels: !ref <n_fea>
#   lin_neurons: !ref <emb_dim>
# 
# classifier: !new:speechbrain.lobes.models.Xvector.Classifier
#   input_shape: [null, null, !ref <emb_dim>]
#   activation: !name:torch.nn.LeakyReLU
#   lin_blocks: 2
#   lin_neurons: !ref <emb_dim>
#   out_neurons: !ref <n_classes>
# 
# epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
#   limit: !ref <number_of_epochs>
# 
# modules:
#   feature_extractor: !ref <feature_extractor>
#   xvector:          !ref <xvector>
#   classifier:       !ref <classifier>
# 
# opt_class: !name:torch.optim.Adam
#   lr: !ref <lr_start>
# 
# lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler
#   initial_value: !ref <lr_start>
#   final_value:   !ref <lr_final>
#   epoch_count:   !ref <number_of_epochs>
# 
# checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#   checkpoints_dir: !ref <save_folder>
#   recoverables:
#     feature_extractor: !ref <feature_extractor>
#     xvector:          !ref <xvector>
#     classifier:       !ref <classifier>
#     counter:          !ref <epoch_counter>
# 
# pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
#   collect_in: !ref <save_folder>
#   loadables:
#     xvector: !ref <xvector>
#   paths:
#     xvector: /content/xvector_best.ckpt

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file xvector_train.py
# #!/usr/bin/env python3
# 
# import os, sys
# import torch, torchaudio
# import speechbrain as sb
# from hyperpyyaml import load_hyperpyyaml
# 
# class SpeakerBrain(sb.Brain):
#   def compute_forward(self, batch, stage):
#     batch = batch.to(self.device)
#     wavs, lengths = batch.sig
# 
#     # if stage == sb.Stage.TRAIN:
#     #   wavs = hparams["noise_transform"](wavs, lengths)
#     #   wavs = hparams["speed_transform"](wavs)
# 
#     feats = self.modules.feature_extractor(wavs)
#     _, lens = batch.sig
#     embeddings = self.modules.xvector(feats, lens)
#     predictions = self.modules.classifier(embeddings)
#     return predictions
# 
#   def compute_objectives(self, predictions, batch, stage):
#     _, lens = batch.sig
#     labels, _ = batch.label_encoded
#     loss = sb.nnet.losses.nll_loss(predictions, labels, lens)
#     self.loss_metric.append(batch.id, predictions, labels, lens, reduction="batch")
#     if stage != sb.Stage.TRAIN:
#         self.error_metrics.append(batch.id, predictions, labels, lens)
#     return loss
# 
#   def on_stage_start(self, stage, epoch=None):
#     self.loss_metric = sb.utils.metric_stats.MetricStats(
#       metric=sb.nnet.losses.nll_loss
#     )
#     if stage != sb.Stage.TRAIN:
#       self.error_metrics = self.hparams.error_stats()
# 
#   def on_stage_end(self, stage, stage_loss, epoch=None):
#     if stage == sb.Stage.TRAIN:
#       self.train_loss = stage_loss
#     else:
#       stats = {
#         "loss": stage_loss,
#         "error": self.error_metrics.summarize("average"),
#       }
#     if stage == sb.Stage.VALID:
#       old_lr, new_lr = self.hparams.lr_annealing(epoch)
#       sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)
#       self.hparams.train_logger.log_stats(
#         {"Epoch": epoch, "lr": old_lr},
#         train_stats={"loss": self.train_loss},
#         valid_stats=stats,
#       )
#       self.checkpointer.save_and_keep_only(meta=stats, min_keys=["error"])
#     if stage == sb.Stage.TEST:
#       self.hparams.train_logger.log_stats(
#         {"Epoch loaded": self.hparams.epoch_counter.current},
#         test_stats=stats,
#       )
# 
# def random_crop(sig, sr, max_dur):
#   max_len = int(sr * max_dur)
#   if sig.shape[0] > max_len:
#     start = torch.randint(0, sig.shape[0] - max_len + 1, (1,)).item()
#     sig = sig[start : start + max_len]
#   else:
#     pad = max_len - sig.shape[0]
#     sig = torch.nn.functional.pad(sig, (0, pad))
#   return sig
# 
# def dataio_prep(hparams):
#   label_encoder = sb.dataio.encoder.CategoricalEncoder()
# 
#   @sb.utils.data_pipeline.takes("label")
#   @sb.utils.data_pipeline.provides("label", "label_encoded")
#   def label_pipeline(label):
#     yield label
#     yield label_encoder.encode_label_torch(label)
# 
#   @sb.utils.data_pipeline.takes("wav")
#   @sb.utils.data_pipeline.provides("sig")
#   def audio_pipeline(wav):
# 
#     sig = sb.dataio.dataio.read_audio(wav)
# 
#     # Resample
#     sig = torchaudio.functional.resample(
#       sig,
#       orig_freq=hparams["orig_sample_rate"],
#       new_freq=hparams["sample_rate"],
#     )
# 
#     # Random crop to fixed duration
#     sig = random_crop(
#         sig,
#         hparams["sample_rate"],
#         hparams["chunk_duration"]
#     )
# 
#     # Normalize
#     sig = sig / sig.abs().max()
# 
#     return sig
# 
#   hparams["dataloader_options"]["shuffle"] = True
# 
#   data_json = {
#     "train": hparams["train_annotation"],
#     "valid": hparams["valid_annotation"],
#     "test": hparams["test_annotation"],
#   }
#   datasets = {}
#   for name, path in data_json.items():
#     datasets[name] = sb.dataio.dataset.DynamicItemDataset.from_json(
#       json_path=path,
#       dynamic_items=[audio_pipeline, label_pipeline],
#       output_keys=["id", "sig", "label_encoded"],
#     )
# 
#   lab_enc_file = os.path.join(hparams["save_folder"], "label_encoder.txt")
#   label_encoder.load_or_create(
#     path=lab_enc_file, from_didatasets=[datasets["train"]], output_key="label"
#   )
#   label_encoder.expect_len(hparams["n_classes"])
#   return datasets
# 
# if __name__ == "__main__":
# 
#     hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])
# 
#     with open(hparams_file) as fin:
#       hparams = load_hyperpyyaml(fin, overrides)
# 
#     sb.create_experiment_directory(
#       experiment_directory=hparams["output_folder"],
#       hyperparams_to_save=hparams_file,
#       overrides=overrides,
#     )
# 
#     datasets = dataio_prep(hparams)
# 
#     speaker_brain = SpeakerBrain(
#       modules=hparams["modules"],
#       opt_class=hparams["opt_class"],
#       hparams=hparams,
#       run_opts=run_opts,
#       checkpointer=hparams["checkpointer"],
#     )
# 
#     speaker_brain.fit(
#       epoch_counter=speaker_brain.hparams.epoch_counter,
#       train_set=datasets["train"],
#       valid_set=datasets["valid"],
#       train_loader_kwargs=hparams["dataloader_options"],
#       valid_loader_kwargs=hparams["dataloader_options"],
#     )
# 
#     speaker_brain.evaluate(
#       test_set=datasets["test"],
#       test_loader_kwargs=hparams["dataloader_options"],
#     )

"""### Escapa-TDNN"""

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file escapa_tdnn_hparams.yaml
# 
# seed: 1968
# __set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]
# 
# project_folder: !PLACEHOLDER
# output_folder: !ref <project_folder>/results/escapa_tdnn/<seed>
# save_folder: !ref <output_folder>/save
# train_log: !ref <output_folder>/train_log.txt
# 
# train_annotation: !ref <project_folder>/train.json
# valid_annotation: !ref <project_folder>/valid.json
# test_annotation:  !ref <project_folder>/test.json
# 
# # The train logger writes training statistics to a file, as well as stdout.
# train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
#   save_file: !ref <train_log>
# 
# error_stats: !name:speechbrain.utils.metric_stats.MetricStats
#   metric: !name:speechbrain.nnet.losses.classification_error
#     reduction: batch
# 
# # Checkpoint every 15min
# ckpt_interval_minutes: 15
# 
# # Training Parameters
# number_of_epochs: 30
# batch_size: 16
# grad_accumulation_factor: 2
# lr: 0.0001
# weight_decay: 0.00002
# base_lr: 0.000001
# max_lr: !ref <lr>
# step_size: 1088
# mode: exp_range
# gamma: 0.9998
# orig_sample_rate: 16000
# sample_rate: 8000
# chunk_duration: 20.0
# shuffle: True
# random_chunk: True
# drop_last: False
# 
# # Feature extraction
# n_mels: 80
# left_frames: 0
# right_frames: 0
# deltas: False
# 
# # Number of Classes
# out_n_neurons: 2
# 
# dataloader_options:
#   batch_size: !ref <batch_size>
#   shuffle: !ref <shuffle>
#   num_workers: 2
#   drop_last: !ref <drop_last>
# 
# # Functions
# compute_features: !new:speechbrain.lobes.features.Fbank
#   n_mels: !ref <n_mels>
#   left_frames: !ref <left_frames>
#   right_frames: !ref <right_frames>
#   deltas: !ref <deltas>
# 
# embedding_model: !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
#   input_size: !ref <n_mels>
#   channels: [512, 512, 512, 512, 1536]
#   kernel_sizes: [5, 3, 3, 3, 1]
#   dilations: [1, 2, 3, 4, 1]
#   attention_channels: 64
#   lin_neurons: 96
# 
# classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier
#   input_size: 96
#   out_neurons: !ref <out_n_neurons>
# 
# epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
#   limit: !ref <number_of_epochs>
# 
# mean_var_norm: !new:speechbrain.processing.features.InputNormalization
#   norm_type: sentence
#   std_norm: False
# 
# modules:
#   compute_features: !ref <compute_features>
#   embedding_model: !ref <embedding_model>
#   classifier: !ref <classifier>
#   mean_var_norm: !ref <mean_var_norm>
# 
# compute_cost: !new:speechbrain.nnet.losses.LogSoftmaxWrapper
#   loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin
#     margin: 0.2
#     scale: 30
# 
# compute_error: !name:speechbrain.nnet.losses.classification_error
# 
# opt_class: !name:torch.optim.Adam
#   lr: !ref <lr>
#   weight_decay: !ref <weight_decay>
# 
# lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler
#   mode: !ref <mode>
#   gamma: !ref <gamma>
#   base_lr: !ref <base_lr>
#   max_lr: !ref <max_lr>
#   step_size: !ref <step_size>
# 
# 
# checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#   checkpoints_dir: !ref <save_folder>
#   recoverables:
#     embedding_model: !ref <embedding_model>
#     classifier: !ref <classifier>
#     normalizer: !ref <mean_var_norm>
#     counter: !ref <epoch_counter>

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file ecapa_tdnn_train.py
# #!/usr/bin/env python3
# 
# import os, sys
# import torch, torchaudio
# import speechbrain as sb
# from speechbrain.utils.distributed import run_on_main
# from speechbrain.dataio.dataio import length_to_mask
# from hyperpyyaml import load_hyperpyyaml
# 
# class SpeakerBrain(sb.Brain):
#   def compute_forward(self, batch, stage):
#     batch = batch.to(self.device)
#     wavs, lens = batch.sig
# 
#     # Feature extraction and normalization
#     feats = self.modules.compute_features(wavs)
#     feats = self.modules.mean_var_norm(feats, lens)
# 
#     # Embeddings + speaker classifier
#     embeddings = self.modules.embedding_model(feats, lens)
#     outputs = self.modules.classifier(embeddings)
# 
#     return outputs
# 
#   def compute_objectives(self, predictions, batch, stage):
#     _, lens = batch.sig
#     label, _ = batch.label_encoded
# 
#     # Concatenate labels (due to data augmentation)
#     if stage == sb.Stage.TRAIN:
#       if hasattr(self.hparams.lr_annealing, "on_batch_end"):
#         self.hparams.lr_annealing.on_batch_end(self.optimizer)
# 
#     loss = self.hparams.compute_cost(predictions, label, lens)
# 
#     if stage != sb.Stage.TRAIN:
#       self.error_metrics.append(batch.id, predictions, label, lens)
# 
#     return loss
# 
#   def on_stage_start(self, stage, epoch=None):
# 
#     self.loss_metric = sb.utils.metric_stats.MetricStats(
#       metric=sb.nnet.losses.nll_loss
#     )
# 
#     if stage != sb.Stage.TRAIN:
#       self.error_metrics = self.hparams.error_stats()
# 
#   def on_stage_end(self, stage, stage_loss, epoch=None):
# 
#     if stage == sb.Stage.TRAIN:
#       self.train_loss = stage_loss
# 
#     else:
#       stats = {
#           "loss": stage_loss,
#           "error": self.error_metrics.summarize("average")
#       }
# 
#     if stage == sb.Stage.VALID:
# 
#       old_lr, new_lr = self.hparams.lr_annealing(epoch)
#       sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)
# 
#       self.hparams.train_logger.log_stats(
#           {"Epoch": epoch, "lr": old_lr},
#           train_stats={"loss": self.train_loss},
#           valid_stats=stats,
#       )
# 
#       self.checkpointer.save_and_keep_only(meta=stats, min_keys=["error"])
# 
#     if stage == sb.Stage.TEST:
#         self.hparams.train_logger.log_stats(
#             {"Epoch loaded": self.hparams.epoch_counter.current},
#             test_stats=stats,
#         )
# 
# def random_crop(sig, sr, max_dur):
#   max_len = int(sr * max_dur)
#   if sig.shape[0] > max_len:
#     start = torch.randint(0, sig.shape[0] - max_len + 1, (1,)).item()
#     sig = sig[start : start + max_len]
#   else:
#     pad = max_len - sig.shape[0]
#     sig = torch.nn.functional.pad(sig, (0, pad))
#   return sig
# 
# def dataio_prep(hparams):
#   label_encoder = sb.dataio.encoder.CategoricalEncoder()
# 
#   @sb.utils.data_pipeline.takes("label")
#   @sb.utils.data_pipeline.provides("label", "label_encoded")
#   def label_pipeline(label):
#     yield label
#     yield label_encoder.encode_label_torch(label)
# 
#   @sb.utils.data_pipeline.takes("wav")
#   @sb.utils.data_pipeline.provides("sig")
#   def audio_pipeline(wav):
# 
#     sig = sb.dataio.dataio.read_audio(wav)
# 
#     # Resample
#     sig = torchaudio.functional.resample(
#       sig,
#       orig_freq=hparams["orig_sample_rate"],
#       new_freq=hparams["sample_rate"],
#     )
# 
#     # Random crop to fixed duration
#     sig = random_crop(
#         sig,
#         hparams["sample_rate"],
#         hparams["chunk_duration"]
#     )
# 
#     # Normalize
#     sig = sig / sig.abs().max()
# 
#     return sig
# 
#   hparams["dataloader_options"]["shuffle"] = True
# 
#   data_json = {
#     "train": hparams["train_annotation"],
#     "valid": hparams["valid_annotation"],
#     "test": hparams["test_annotation"],
#   }
#   datasets = {}
#   for name, path in data_json.items():
#     datasets[name] = sb.dataio.dataset.DynamicItemDataset.from_json(
#       json_path=path,
#       dynamic_items=[audio_pipeline, label_pipeline],
#       output_keys=["id", "sig", "label_encoded"],
#     )
# 
#   lab_enc_file = os.path.join(hparams["save_folder"], "label_encoder.txt")
#   label_encoder.load_or_create(
#     path=lab_enc_file, from_didatasets=[datasets["train"]], output_key="label"
#   )
#   label_encoder.expect_len(hparams["out_n_neurons"])
#   return datasets
# 
# if __name__ == "__main__":
#     hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])
#     with open(hparams_file) as fin:
#         hparams = load_hyperpyyaml(fin, overrides)
#     sb.create_experiment_directory(
#         experiment_directory=hparams["output_folder"],
#         hyperparams_to_save=hparams_file,
#         overrides=overrides,
#     )
#     datasets = dataio_prep(hparams)
#     speaker_brain = SpeakerBrain(
#         modules=hparams["modules"],
#         opt_class=hparams["opt_class"],
#         hparams=hparams,
#         run_opts=run_opts,
#         checkpointer=hparams["checkpointer"],
#     )
#     speaker_brain.fit(
#         epoch_counter=speaker_brain.hparams.epoch_counter,
#         train_set=datasets["train"],
#         valid_set=datasets["valid"],
#         train_loader_kwargs=hparams["dataloader_options"],
#         valid_loader_kwargs=hparams["dataloader_options"],
#     )
#     speaker_brain.evaluate(
#         test_set=datasets["test"],
#         test_loader_kwargs=hparams["dataloader_options"],
#     )
#

"""### Wav2Vec2"""

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file wav2vec_hparams.yaml
# 
# seed: 1986
# __set_seed: !apply:torch.manual_seed [!ref <seed>]
# 
# project_folder: !PLACEHOLDER
# output_folder: !ref <project_folder>/results/wav2vec2/<seed>
# save_folder: !ref <output_folder>/save
# train_log: !ref <output_folder>/train_log.txt
# 
# train_annotation: !ref <project_folder>/train.json
# valid_annotation: !ref <project_folder>/valid.json
# test_annotation:  !ref <project_folder>/test.json
# 
# chunk_duration: 20.0
# orig_sample_rate: 16000
# sample_rate: 8000
# 
# sslmodel_hub: facebook/wav2vec2-base-960h
# sslmodel_folder: !ref <save_folder>/ssl_checkpoint
# 
# train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
#   save_file: !ref <train_log>
# 
# number_of_epochs: 30
# batch_size: 16
# lr: 0.0001
# lr_ssl: 0.00001
# 
# freeze_ssl: False
# freeze_ssl_conv: True
# encoder_dim: 768
# 
# out_n_neurons: 2
# 
# noise_transform: !new:speechbrain.augment.time_domain.AddNoise
#   snr_low:  10
#   snr_high: 20
# 
# speed_transform: !new:speechbrain.augment.time_domain.SpeedPerturb
#   orig_freq: !ref <sample_rate>
#   speeds:    [0.9, 1.0, 1.1]
# 
# dataloader_options:
#   batch_size: !ref <batch_size>
#   shuffle: True
#   num_workers: 2
#   drop_last: False
# 
# ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2
#   source: !ref <sslmodel_hub>
#   output_norm: True
#   freeze: !ref <freeze_ssl>
#   freeze_feature_extractor: !ref <freeze_ssl_conv>
#   save_path: !ref <sslmodel_folder>
# 
# avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling
#   return_std: False
# 
# output_mlp: !new:speechbrain.nnet.linear.Linear
#   input_size: !ref <encoder_dim>
#   n_neurons: !ref <out_n_neurons>
#   bias: False
# 
# epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
#   limit: !ref <number_of_epochs>
# 
# modules:
#   ssl_model: !ref <ssl_model>
#   output_mlp: !ref <output_mlp>
# 
# model: !new:torch.nn.ModuleList
#   - [!ref <output_mlp>]
# 
# log_softmax: !new:speechbrain.nnet.activations.Softmax
#   apply_log: True
# 
# compute_cost: !name:speechbrain.nnet.losses.nll_loss
# 
# error_stats: !name:speechbrain.utils.metric_stats.MetricStats
#   metric: !name:speechbrain.nnet.losses.classification_error
#     reduction: batch
# 
# opt_class: !name:torch.optim.Adam
#   lr: !ref <lr>
# 
# ssl_opt_class: !name:torch.optim.Adam
#   lr: !ref <lr_ssl>
# 
# lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
#   initial_value: !ref <lr>
#   improvement_threshold: 0.0025
#   annealing_factor: 0.9
#   patient: 0
# 
# lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler
#   initial_value: !ref <lr_ssl>
#   improvement_threshold: 0.0025
#   annealing_factor: 0.9
# 
# checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#   checkpoints_dir: !ref <save_folder>
#   recoverables:
#     model: !ref <model>
#     ssl_model: !ref <ssl_model>
#     lr_annealing_output: !ref <lr_annealing>
#     lr_annealing_ssl: !ref <lr_annealing_ssl>
#     counter: !ref <epoch_counter>

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file wav2vec2_train.py
# 
# import os
# import sys
# import torch, torchaudio
# import speechbrain as sb
# from hyperpyyaml import load_hyperpyyaml
# import torchaudio.transforms as T
# 
# 
# class LanguageBrain(sb.Brain):
#   def compute_forward(self, batch, stage):
#     batch = batch.to(self.device)
#     wavs, lens = batch.sig
# 
#     # if stage == sb.Stage.TRAIN:
#     #   wavs = hparams["noise_transform"](wavs, lengths)
#     #   wavs = hparams["speed_transform"](wavs)
# 
#     outputs = self.modules.ssl_model(wavs, lens)
# 
# 
#     outputs = self.hparams.avg_pool(outputs, lens)
#     outputs = outputs.view(outputs.shape[0], -1)
# 
#     outputs = self.modules.output_mlp(outputs)
#     outputs = self.hparams.log_softmax(outputs)
#     return outputs
# 
#   def compute_objectives(self, predictions, batch, stage):
#     labels, _ = batch.label_encoded
# 
#     """to meet the input form of nll loss"""
#     labels = labels.squeeze(1)
#     loss = self.hparams.compute_cost(predictions, labels)
#     if stage != sb.Stage.TRAIN:
#         self.error_metrics.append(batch.label, predictions, labels)
# 
#     return loss
# 
#   def on_stage_start(self, stage, epoch=None):
# 
#     self.loss_metric = sb.utils.metric_stats.MetricStats(
#       metric=sb.nnet.losses.nll_loss
#     )
# 
#     if stage != sb.Stage.TRAIN:
#       self.error_metrics = self.hparams.error_stats()
# 
#   def on_stage_end(self, stage, stage_loss, epoch=None):
# 
#     if stage == sb.Stage.TRAIN:
#       self.train_loss = stage_loss
# 
#     else:
#       stats = {
#         "loss": stage_loss,
#         "error_rate": self.error_metrics.summarize("average"),
#       }
# 
#     if stage == sb.Stage.VALID:
# 
#       old_lr, new_lr = self.hparams.lr_annealing(stats["error_rate"])
#       sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)
# 
#       (
#         old_lr_ssl,
#         new_lr_ssl,
#       ) = self.hparams.lr_annealing_ssl(stats["error_rate"])
# 
#       sb.nnet.schedulers.update_learning_rate(
#         self.ssl_optimizer, new_lr_ssl
#       )
# 
#       self.hparams.train_logger.log_stats(
#         {"Epoch": epoch, "lr": old_lr, "ssl_lr": old_lr_ssl},
#         train_stats={"loss": self.train_loss},
#         valid_stats=stats,
#       )
# 
#       self.checkpointer.save_and_keep_only(
#         meta=stats, min_keys=["error_rate"]
#       )
# 
#     if stage == sb.Stage.TEST:
#       self.hparams.train_logger.log_stats(
#         {"Epoch loaded": self.hparams.epoch_counter.current},
#         test_stats=stats,
#       )
# 
#     torch.cuda.empty_cache()
# 
#   def init_optimizers(self):
#     "Initializes the ssl optimizer and model optimizer"
#     self.ssl_optimizer = self.hparams.ssl_opt_class(
#       self.modules.ssl_model.parameters()
#     )
#     self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())
# 
#     if self.checkpointer is not None:
#       self.checkpointer.add_recoverable(
#           "ssl_opt", self.ssl_optimizer
#       )
#       self.checkpointer.add_recoverable("optimizer", self.optimizer)
# 
#     self.optimizers_dict = {
#       "model_optimizer": self.optimizer,
#       "ssl_optimizer": self.ssl_optimizer,
#     }
# 
# def random_crop(sig, sr, max_dur):
#   max_len = int(sr * max_dur)
#   if sig.shape[0] > max_len:
#     start = torch.randint(0, sig.shape[0] - max_len + 1, (1,)).item()
#     sig = sig[start : start + max_len]
#   else:
#     pad = max_len - sig.shape[0]
#     sig = torch.nn.functional.pad(sig, (0, pad))
#   return sig
# 
# def dataio_prep(hparams):
# 
#   @sb.utils.data_pipeline.takes("wav")
#   @sb.utils.data_pipeline.provides("sig")
#   def audio_pipeline(wav):
# 
#     sig = sb.dataio.dataio.read_audio(wav)
# 
#     # Resample
#     sig = torchaudio.functional.resample(
#       sig,
#       orig_freq=hparams["orig_sample_rate"],
#       new_freq=hparams["sample_rate"],
#     )
# 
#     # Random crop to fixed duration
#     sig = random_crop(
#         sig,
#         hparams["sample_rate"],
#         hparams["chunk_duration"]
#     )
# 
#     # Normalize
#     sig = sig / sig.abs().max()
# 
#     return sig
# 
#   @sb.utils.data_pipeline.takes("label")
#   @sb.utils.data_pipeline.provides("label", "label_encoded")
#   def label_pipeline(label):
#     yield label
#     label_encoded = label_encoder.encode_label_torch(label)
#     yield label_encoded
# 
#   label_encoder = sb.dataio.encoder.CategoricalEncoder()
# 
#   data_info = {
#     "train": hparams["train_annotation"],
#     "valid": hparams["valid_annotation"],
#     "test": hparams["test_annotation"],
#   }
# 
#   datasets = {}
# 
#   for dataset in data_info:
# 
#     datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(
#       json_path=data_info[dataset],
#       replacements={"data_root": hparams["project_folder"]},
#       dynamic_items=[audio_pipeline, label_pipeline],
#       output_keys=["label", "sig", "label_encoded"],
#     )
# 
# 
#   lab_enc_file = os.path.join(hparams["save_folder"], "label_encoder.txt")
# 
#   label_encoder.load_or_create(
#     path=lab_enc_file,
#     from_didatasets=[datasets["train"]],
#     output_key="label",
#   )
# 
#   label_encoder.expect_len(hparams["out_n_neurons"])
# 
#   return datasets
# 
# if __name__ == "__main__":
# 
#   hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])
# 
#   # Initialize ddp (useful only for multi-GPU DDP training).
#   sb.utils.distributed.ddp_init_group(run_opts)
# 
#   # Load hyperparameters file with command-line overrides.
#   with open(hparams_file) as fin:
#     hparams = load_hyperpyyaml(fin, overrides)
# 
#   # Create experiment directory
#   sb.create_experiment_directory(
#     experiment_directory=hparams["output_folder"],
#     hyperparams_to_save=hparams_file,
#     overrides=overrides,
#   )
# 
#   # Create dataset objects "train", "valid", and "test".
#   datasets = dataio_prep(hparams)
# 
#   hparams["ssl_model"] = hparams["ssl_model"].to(device=run_opts["device"])
#   # freeze the feature extractor part when unfreezing
#   if not hparams["freeze_ssl"] and hparams["freeze_ssl_conv"]:
#     hparams["ssl_model"].model.feature_extractor._freeze_parameters()
# 
#   # Initialize the Brain object to prepare for mask training.
#   language_brain = LanguageBrain(
#     modules=hparams["modules"],
#     opt_class=hparams["opt_class"],
#     hparams=hparams,
#     run_opts=run_opts,
#     checkpointer=hparams["checkpointer"],
#   )
# 
#   language_brain.fit(
#     epoch_counter=language_brain.hparams.epoch_counter,
#     train_set=datasets["train"],
#     valid_set=datasets["valid"],
#     train_loader_kwargs=hparams["dataloader_options"],
#     valid_loader_kwargs=hparams["dataloader_options"],
#   )
# 
#   # Load the best checkpoint for evaluation
#   test_stats = language_brain.evaluate(
#     test_set=datasets["test"],
#     min_key="error_rate",
#     test_loader_kwargs=hparams["dataloader_options"],
#   )

"""### WavLM"""

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file wavlm_hparams.yaml
# 
# seed: 1986
# __set_seed: !apply:torch.manual_seed [!ref <seed>]
# 
# project_folder: !PLACEHOLDER
# output_folder: !ref <project_folder>/results/wavlm/<seed>
# save_folder: !ref <output_folder>/save
# train_log: !ref <output_folder>/train_log.txt
# 
# train_annotation: !ref <project_folder>/train.json
# valid_annotation: !ref <project_folder>/valid.json
# test_annotation:  !ref <project_folder>/test.json
# 
# chunk_duration: 20.0
# orig_sample_rate: 16000
# sample_rate: 8000
# 
# sslmodel_hub: microsoft/wavlm-base-plus
# sslmodel_folder: !ref <save_folder>/ssl_checkpoint
# 
# train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
#   save_file: !ref <train_log>
# 
# number_of_epochs: 30
# batch_size: 16
# lr: 0.0001
# lr_ssl: 0.00001
# 
# freeze_ssl: False
# freeze_ssl_conv: True
# encoder_dim: 768
# 
# out_n_neurons: 2
# 
# noise_transform: !new:speechbrain.augment.time_domain.AddNoise
#   snr_low:  10
#   snr_high: 20
# 
# speed_transform: !new:speechbrain.augment.time_domain.SpeedPerturb
#   orig_freq: !ref <sample_rate>
#   speeds:    [0.9, 1.0, 1.1]
# 
# dataloader_options:
#   batch_size: !ref <batch_size>
#   shuffle: True
#   num_workers: 2
#   drop_last: False
# 
# ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
#   source: !ref <sslmodel_hub>
#   output_norm: True
#   freeze: !ref <freeze_ssl>
#   freeze_feature_extractor: !ref <freeze_ssl_conv>
#   save_path: !ref <sslmodel_folder>
# 
# avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling
#   return_std: False
# 
# output_mlp: !new:speechbrain.nnet.linear.Linear
#   input_size: !ref <encoder_dim>
#   n_neurons: !ref <out_n_neurons>
#   bias: False
# 
# epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
#   limit: !ref <number_of_epochs>
# 
# modules:
#   ssl_model: !ref <ssl_model>
#   output_mlp: !ref <output_mlp>
# 
# model: !new:torch.nn.ModuleList
#   - [!ref <output_mlp>]
# 
# log_softmax: !new:speechbrain.nnet.activations.Softmax
#   apply_log: True
# 
# compute_cost: !name:speechbrain.nnet.losses.nll_loss
# 
# error_stats: !name:speechbrain.utils.metric_stats.MetricStats
#   metric: !name:speechbrain.nnet.losses.classification_error
#     reduction: batch
# 
# opt_class: !name:torch.optim.Adam
#   lr: !ref <lr>
# 
# ssl_opt_class: !name:torch.optim.Adam
#   lr: !ref <lr_ssl>
# 
# lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
#   initial_value: !ref <lr>
#   improvement_threshold: 0.0025
#   annealing_factor: 0.9
#   patient: 0
# 
# lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler
#   initial_value: !ref <lr_ssl>
#   improvement_threshold: 0.0025
#   annealing_factor: 0.9
# 
# checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#   checkpoints_dir: !ref <save_folder>
#   recoverables:
#     model: !ref <model>
#     ssl_model: !ref <ssl_model>
#     lr_annealing_output: !ref <lr_annealing>
#     lr_annealing_ssl: !ref <lr_annealing_ssl>
#     counter: !ref <epoch_counter>

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file wavlm_train.py
# 
# import os
# import sys
# import torch, torchaudio
# import speechbrain as sb
# from hyperpyyaml import load_hyperpyyaml
# import torchaudio.transforms as T
# 
# 
# class LanguageBrain(sb.Brain):
#   def compute_forward(self, batch, stage):
#     batch = batch.to(self.device)
#     wavs, lens = batch.sig
# 
#     # if stage == sb.Stage.TRAIN:
#     #   wavs = hparams["noise_transform"](wavs, lengths)
#     #   wavs = hparams["speed_transform"](wavs)
# 
#     outputs = self.modules.ssl_model(wavs, lens)
# 
# 
#     outputs = self.hparams.avg_pool(outputs, lens)
#     outputs = outputs.view(outputs.shape[0], -1)
# 
#     outputs = self.modules.output_mlp(outputs)
#     outputs = self.hparams.log_softmax(outputs)
#     return outputs
# 
#   def compute_objectives(self, predictions, batch, stage):
#     labels, _ = batch.label_encoded
# 
#     """to meet the input form of nll loss"""
#     labels = labels.squeeze(1)
#     loss = self.hparams.compute_cost(predictions, labels)
#     if stage != sb.Stage.TRAIN:
#         self.error_metrics.append(batch.label, predictions, labels)
# 
#     return loss
# 
#   def on_stage_start(self, stage, epoch=None):
# 
#     self.loss_metric = sb.utils.metric_stats.MetricStats(
#       metric=sb.nnet.losses.nll_loss
#     )
# 
#     if stage != sb.Stage.TRAIN:
#       self.error_metrics = self.hparams.error_stats()
# 
#   def on_stage_end(self, stage, stage_loss, epoch=None):
# 
#     if stage == sb.Stage.TRAIN:
#       self.train_loss = stage_loss
# 
#     else:
#       stats = {
#         "loss": stage_loss,
#         "error_rate": self.error_metrics.summarize("average"),
#       }
# 
#     if stage == sb.Stage.VALID:
# 
#       old_lr, new_lr = self.hparams.lr_annealing(stats["error_rate"])
#       sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)
# 
#       (
#         old_lr_ssl,
#         new_lr_ssl,
#       ) = self.hparams.lr_annealing_ssl(stats["error_rate"])
# 
#       sb.nnet.schedulers.update_learning_rate(
#         self.ssl_optimizer, new_lr_ssl
#       )
# 
#       self.hparams.train_logger.log_stats(
#         {"Epoch": epoch, "lr": old_lr, "ssl_lr": old_lr_ssl},
#         train_stats={"loss": self.train_loss},
#         valid_stats=stats,
#       )
# 
#       self.checkpointer.save_and_keep_only(
#         meta=stats, min_keys=["error_rate"]
#       )
# 
#     if stage == sb.Stage.TEST:
#       self.hparams.train_logger.log_stats(
#         {"Epoch loaded": self.hparams.epoch_counter.current},
#         test_stats=stats,
#       )
# 
#     torch.cuda.empty_cache()
# 
#   def init_optimizers(self):
#     "Initializes the ssl optimizer and model optimizer"
#     self.ssl_optimizer = self.hparams.ssl_opt_class(
#       self.modules.ssl_model.parameters()
#     )
#     self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())
# 
#     if self.checkpointer is not None:
#       self.checkpointer.add_recoverable(
#           "ssl_opt", self.ssl_optimizer
#       )
#       self.checkpointer.add_recoverable("optimizer", self.optimizer)
# 
#     self.optimizers_dict = {
#       "model_optimizer": self.optimizer,
#       "ssl_optimizer": self.ssl_optimizer,
#     }
# 
# def random_crop(sig, sr, max_dur):
#   max_len = int(sr * max_dur)
#   if sig.shape[0] > max_len:
#     start = torch.randint(0, sig.shape[0] - max_len + 1, (1,)).item()
#     sig = sig[start : start + max_len]
#   else:
#     pad = max_len - sig.shape[0]
#     sig = torch.nn.functional.pad(sig, (0, pad))
#   return sig
# 
# def dataio_prep(hparams):
# 
#   @sb.utils.data_pipeline.takes("wav")
#   @sb.utils.data_pipeline.provides("sig")
#   def audio_pipeline(wav):
# 
#     sig = sb.dataio.dataio.read_audio(wav)
# 
#     # Resample
#     sig = torchaudio.functional.resample(
#       sig,
#       orig_freq=hparams["orig_sample_rate"],
#       new_freq=hparams["sample_rate"],
#     )
# 
#     # Random crop to fixed duration
#     sig = random_crop(
#         sig,
#         hparams["sample_rate"],
#         hparams["chunk_duration"]
#     )
# 
#     # Normalize
#     sig = sig / sig.abs().max()
# 
#     return sig
# 
#   @sb.utils.data_pipeline.takes("label")
#   @sb.utils.data_pipeline.provides("label", "label_encoded")
#   def label_pipeline(label):
#     yield label
#     label_encoded = label_encoder.encode_label_torch(label)
#     yield label_encoded
# 
#   label_encoder = sb.dataio.encoder.CategoricalEncoder()
# 
#   data_info = {
#     "train": hparams["train_annotation"],
#     "valid": hparams["valid_annotation"],
#     "test": hparams["test_annotation"],
#   }
# 
#   datasets = {}
# 
#   for dataset in data_info:
# 
#     datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(
#       json_path=data_info[dataset],
#       replacements={"data_root": hparams["project_folder"]},
#       dynamic_items=[audio_pipeline, label_pipeline],
#       output_keys=["label", "sig", "label_encoded"],
#     )
# 
# 
#   lab_enc_file = os.path.join(hparams["save_folder"], "label_encoder.txt")
# 
#   label_encoder.load_or_create(
#     path=lab_enc_file,
#     from_didatasets=[datasets["train"]],
#     output_key="label",
#   )
# 
#   label_encoder.expect_len(hparams["out_n_neurons"])
# 
#   return datasets
# 
# if __name__ == "__main__":
# 
#   hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])
# 
#   # Initialize ddp (useful only for multi-GPU DDP training).
#   sb.utils.distributed.ddp_init_group(run_opts)
# 
#   # Load hyperparameters file with command-line overrides.
#   with open(hparams_file) as fin:
#     hparams = load_hyperpyyaml(fin, overrides)
# 
#   # Create experiment directory
#   sb.create_experiment_directory(
#     experiment_directory=hparams["output_folder"],
#     hyperparams_to_save=hparams_file,
#     overrides=overrides,
#   )
# 
#   # Create dataset objects "train", "valid", and "test".
#   datasets = dataio_prep(hparams)
# 
#   hparams["ssl_model"] = hparams["ssl_model"].to(device=run_opts["device"])
#   # freeze the feature extractor part when unfreezing
#   if not hparams["freeze_ssl"] and hparams["freeze_ssl_conv"]:
#     hparams["ssl_model"].model.feature_extractor._freeze_parameters()
# 
#   # Initialize the Brain object to prepare for mask training.
#   language_brain = LanguageBrain(
#     modules=hparams["modules"],
#     opt_class=hparams["opt_class"],
#     hparams=hparams,
#     run_opts=run_opts,
#     checkpointer=hparams["checkpointer"],
#   )
# 
#   language_brain.fit(
#     epoch_counter=language_brain.hparams.epoch_counter,
#     train_set=datasets["train"],
#     valid_set=datasets["valid"],
#     train_loader_kwargs=hparams["dataloader_options"],
#     valid_loader_kwargs=hparams["dataloader_options"],
#   )
# 
#   # Load the best checkpoint for evaluation
#   test_stats = language_brain.evaluate(
#     test_set=datasets["test"],
#     min_key="error_rate",
#     test_loader_kwargs=hparams["dataloader_options"],
#   )

"""### Hubert"""

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file hubert_hparams.yaml
# 
# seed: 1986
# __set_seed: !apply:torch.manual_seed [!ref <seed>]
# 
# project_folder: !PLACEHOLDER
# output_folder: !ref <project_folder>/results/hubert/<seed>
# save_folder: !ref <output_folder>/save
# train_log: !ref <output_folder>/train_log.txt
# 
# train_annotation: !ref <project_folder>/train.json
# valid_annotation: !ref <project_folder>/valid.json
# test_annotation:  !ref <project_folder>/test.json
# 
# chunk_duration: 20.0
# orig_sample_rate: 16000
# sample_rate: 8000
# 
# sslmodel_hub: facebook/hubert-base-ls960
# sslmodel_folder: !ref <save_folder>/ssl_checkpoint
# 
# train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
#   save_file: !ref <train_log>
# 
# number_of_epochs: 30
# batch_size: 16
# lr: 0.0001
# lr_ssl: 0.00001
# 
# freeze_ssl: False
# freeze_ssl_conv: True
# encoder_dim: 768
# 
# out_n_neurons: 2
# 
# noise_transform: !new:speechbrain.augment.time_domain.AddNoise
#   snr_low:  10
#   snr_high: 20
# 
# speed_transform: !new:speechbrain.augment.time_domain.SpeedPerturb
#   orig_freq: !ref <sample_rate>
#   speeds:    [0.9, 1.0, 1.1]
# 
# dataloader_options:
#   batch_size: !ref <batch_size>
#   shuffle: True
#   num_workers: 2
#   drop_last: False
# 
# ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT
#   source: !ref <sslmodel_hub>
#   output_norm: True
#   freeze: !ref <freeze_ssl>
#   freeze_feature_extractor: !ref <freeze_ssl_conv>
#   save_path: !ref <sslmodel_folder>
#   apply_spec_augment: False
#   output_all_hiddens: False
# 
# avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling
#   return_std: False
# 
# output_mlp: !new:speechbrain.nnet.linear.Linear
#   input_size: !ref <encoder_dim>
#   n_neurons: !ref <out_n_neurons>
#   bias: False
# 
# epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
#   limit: !ref <number_of_epochs>
# 
# modules:
#   ssl_model: !ref <ssl_model>
#   output_mlp: !ref <output_mlp>
# 
# model: !new:torch.nn.ModuleList
#   - [!ref <output_mlp>]
# 
# log_softmax: !new:speechbrain.nnet.activations.Softmax
#   apply_log: True
# 
# compute_cost: !name:speechbrain.nnet.losses.nll_loss
# 
# error_stats: !name:speechbrain.utils.metric_stats.MetricStats
#   metric: !name:speechbrain.nnet.losses.classification_error
#     reduction: batch
# 
# opt_class: !name:torch.optim.Adam
#   lr: !ref <lr>
# 
# ssl_opt_class: !name:torch.optim.Adam
#   lr: !ref <lr_ssl>
# 
# lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler
#   initial_value: !ref <lr>
#   improvement_threshold: 0.0025
#   annealing_factor: 0.9
#   patient: 0
# 
# lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler
#   initial_value: !ref <lr_ssl>
#   improvement_threshold: 0.0025
#   annealing_factor: 0.9
# 
# checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#   checkpoints_dir: !ref <save_folder>
#   recoverables:
#     model: !ref <model>
#     ssl_model: !ref <ssl_model>
#     lr_annealing_output: !ref <lr_annealing>
#     lr_annealing_ssl: !ref <lr_annealing_ssl>
#     counter: !ref <epoch_counter>

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%file hubert_train.py
# 
# import os
# import sys
# import torch, torchaudio
# import speechbrain as sb
# from hyperpyyaml import load_hyperpyyaml
# import torchaudio.transforms as T
# 
# 
# class LanguageBrain(sb.Brain):
#   def compute_forward(self, batch, stage):
#     batch = batch.to(self.device)
#     wavs, lens = batch.sig
# 
#     # if stage == sb.Stage.TRAIN:
#     #   wavs = hparams["noise_transform"](wavs, lengths)
#     #   wavs = hparams["speed_transform"](wavs)
# 
#     outputs = self.modules.ssl_model(wavs, lens)
# 
# 
#     outputs = self.hparams.avg_pool(outputs, lens)
#     outputs = outputs.view(outputs.shape[0], -1)
# 
#     outputs = self.modules.output_mlp(outputs)
#     outputs = self.hparams.log_softmax(outputs)
#     return outputs
# 
#   def compute_objectives(self, predictions, batch, stage):
#     labels, _ = batch.label_encoded
# 
#     """to meet the input form of nll loss"""
#     labels = labels.squeeze(1)
#     loss = self.hparams.compute_cost(predictions, labels)
#     if stage != sb.Stage.TRAIN:
#         self.error_metrics.append(batch.label, predictions, labels)
# 
#     return loss
# 
#   def on_stage_start(self, stage, epoch=None):
# 
#     self.loss_metric = sb.utils.metric_stats.MetricStats(
#       metric=sb.nnet.losses.nll_loss
#     )
# 
#     if stage != sb.Stage.TRAIN:
#       self.error_metrics = self.hparams.error_stats()
# 
#   def on_stage_end(self, stage, stage_loss, epoch=None):
# 
#     if stage == sb.Stage.TRAIN:
#       self.train_loss = stage_loss
# 
#     else:
#       stats = {
#         "loss": stage_loss,
#         "error_rate": self.error_metrics.summarize("average"),
#       }
# 
#     if stage == sb.Stage.VALID:
# 
#       old_lr, new_lr = self.hparams.lr_annealing(stats["error_rate"])
#       sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)
# 
#       (
#         old_lr_ssl,
#         new_lr_ssl,
#       ) = self.hparams.lr_annealing_ssl(stats["error_rate"])
# 
#       sb.nnet.schedulers.update_learning_rate(
#         self.ssl_optimizer, new_lr_ssl
#       )
# 
#       self.hparams.train_logger.log_stats(
#         {"Epoch": epoch, "lr": old_lr, "ssl_lr": old_lr_ssl},
#         train_stats={"loss": self.train_loss},
#         valid_stats=stats,
#       )
# 
#       self.checkpointer.save_and_keep_only(
#         meta=stats, min_keys=["error_rate"]
#       )
# 
#     if stage == sb.Stage.TEST:
#       self.hparams.train_logger.log_stats(
#         {"Epoch loaded": self.hparams.epoch_counter.current},
#         test_stats=stats,
#       )
# 
#     torch.cuda.empty_cache()
# 
#   def init_optimizers(self):
#     "Initializes the ssl optimizer and model optimizer"
#     self.ssl_optimizer = self.hparams.ssl_opt_class(
#       self.modules.ssl_model.parameters()
#     )
#     self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())
# 
#     if self.checkpointer is not None:
#       self.checkpointer.add_recoverable(
#           "ssl_opt", self.ssl_optimizer
#       )
#       self.checkpointer.add_recoverable("optimizer", self.optimizer)
# 
#     self.optimizers_dict = {
#       "model_optimizer": self.optimizer,
#       "ssl_optimizer": self.ssl_optimizer,
#     }
# 
# def random_crop(sig, sr, max_dur):
#   max_len = int(sr * max_dur)
#   if sig.shape[0] > max_len:
#     start = torch.randint(0, sig.shape[0] - max_len + 1, (1,)).item()
#     sig = sig[start : start + max_len]
#   else:
#     pad = max_len - sig.shape[0]
#     sig = torch.nn.functional.pad(sig, (0, pad))
#   return sig
# 
# def dataio_prep(hparams):
# 
#   @sb.utils.data_pipeline.takes("wav")
#   @sb.utils.data_pipeline.provides("sig")
#   def audio_pipeline(wav):
# 
#     sig = sb.dataio.dataio.read_audio(wav)
# 
#     # Resample
#     sig = torchaudio.functional.resample(
#       sig,
#       orig_freq=hparams["orig_sample_rate"],
#       new_freq=hparams["sample_rate"],
#     )
# 
#     # Random crop to fixed duration
#     sig = random_crop(
#         sig,
#         hparams["sample_rate"],
#         hparams["chunk_duration"]
#     )
# 
#     # Normalize
#     sig = sig / sig.abs().max()
# 
#     return sig
# 
#   @sb.utils.data_pipeline.takes("label")
#   @sb.utils.data_pipeline.provides("label", "label_encoded")
#   def label_pipeline(label):
#     yield label
#     label_encoded = label_encoder.encode_label_torch(label)
#     yield label_encoded
# 
#   label_encoder = sb.dataio.encoder.CategoricalEncoder()
# 
#   data_info = {
#     "train": hparams["train_annotation"],
#     "valid": hparams["valid_annotation"],
#     "test": hparams["test_annotation"],
#   }
# 
#   datasets = {}
# 
#   for dataset in data_info:
# 
#     datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(
#       json_path=data_info[dataset],
#       replacements={"data_root": hparams["project_folder"]},
#       dynamic_items=[audio_pipeline, label_pipeline],
#       output_keys=["label", "sig", "label_encoded"],
#     )
# 
# 
#   lab_enc_file = os.path.join(hparams["save_folder"], "label_encoder.txt")
# 
#   label_encoder.load_or_create(
#     path=lab_enc_file,
#     from_didatasets=[datasets["train"]],
#     output_key="label",
#   )
# 
#   label_encoder.expect_len(hparams["out_n_neurons"])
# 
#   return datasets
# 
# if __name__ == "__main__":
# 
#   hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])
# 
#   # Initialize ddp (useful only for multi-GPU DDP training).
#   sb.utils.distributed.ddp_init_group(run_opts)
# 
#   # Load hyperparameters file with command-line overrides.
#   with open(hparams_file) as fin:
#     hparams = load_hyperpyyaml(fin, overrides)
# 
#   # Create experiment directory
#   sb.create_experiment_directory(
#     experiment_directory=hparams["output_folder"],
#     hyperparams_to_save=hparams_file,
#     overrides=overrides,
#   )
# 
#   # Create dataset objects "train", "valid", and "test".
#   datasets = dataio_prep(hparams)
# 
#   hparams["ssl_model"] = hparams["ssl_model"].to(device=run_opts["device"])
#   # freeze the feature extractor part when unfreezing
#   if not hparams["freeze_ssl"] and hparams["freeze_ssl_conv"]:
#     hparams["ssl_model"].model.feature_extractor._freeze_parameters()
# 
#   # Initialize the Brain object to prepare for mask training.
#   language_brain = LanguageBrain(
#     modules=hparams["modules"],
#     opt_class=hparams["opt_class"],
#     hparams=hparams,
#     run_opts=run_opts,
#     checkpointer=hparams["checkpointer"],
#   )
# 
#   language_brain.fit(
#     epoch_counter=language_brain.hparams.epoch_counter,
#     train_set=datasets["train"],
#     valid_set=datasets["valid"],
#     train_loader_kwargs=hparams["dataloader_options"],
#     valid_loader_kwargs=hparams["dataloader_options"],
#   )
# 
#   # Load the best checkpoint for evaluation
#   test_stats = language_brain.evaluate(
#     test_set=datasets["test"],
#     min_key="error_rate",
#     test_loader_kwargs=hparams["dataloader_options"],
#   )

"""# Compiling Code

## Running Instances
"""

# !rm -rf "/content/drive/MyDrive/Colab Notebooks/Con. AI COMP 691/Con. AI Project/results/xvector/1986"

!python /content/xvector_train.py /content/xvector_hparams.yaml --device="{DEVICE}" --project_folder="{PROJECT_PATH}"

# !rm -rf "/content/drive/MyDrive/Colab Notebooks/Con. AI COMP 691/Con. AI Project/results/ecapa_tdnn/1986"

!python /content/ecapa_tdnn_train.py /content/escapa_tdnn_hparams.yaml --device="{DEVICE}" --project_folder="{PROJECT_PATH}"

# !rm -rf "/content/drive/MyDrive/Colab Notebooks/Con. AI COMP 691/Con. AI Project/results/wav2vec2/1986"

!python /content/wav2vec2_train.py /content/wav2vec_hparams.yaml --device="{DEVICE}" --project_folder="{PROJECT_PATH}"

# !rm -rf "/content/drive/MyDrive/Colab Notebooks/Con. AI COMP 691/Con. AI Project/results/wavlm/1986"

!python /content/wavlm_train.py /content/wavlm_hparams.yaml --device="{DEVICE}" --project_folder="{PROJECT_PATH}"

# !rm -rf "/content/drive/MyDrive/Colab Notebooks/Con. AI COMP 691/Con. AI Project/results/hubert/1986"

!python /content/hubert_train.py /content/hubert_hparams.yaml --device="{DEVICE}" --project_folder="{PROJECT_PATH}"

"""# Refrences

## Dataset

```
@data{aw6b-tg17-19,
  doi = {10.21227/aw6b-tg17},
  url = {https://dx.doi.org/10.21227/aw6b-tg17},
  author = {Dimauro, Giovanni and Girardi, Francesco},
  publisher = {IEEE Dataport},
  title = {Italian Parkinson's Voice and Speech},
  year = {2019}
}
```
"""

